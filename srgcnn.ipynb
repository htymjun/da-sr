{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l2yMUy1lRuoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c4c606-5b88-4b6e-f451-6cab0cd62bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'da-sr'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 85 (delta 47), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (85/85), 10.07 MiB | 5.45 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "!git clone https://github.com/htymjun/da-sr\n",
        "os.chdir('/content/da-sr')\n",
        "\n",
        "from read import fw_to_np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read cn and fw files\n",
        "u, v, w, p, t, icount, x, y, z = fw_to_np('data')\n",
        "# interpolate\n",
        "uc = 0.5e0*(u[:-1,:,:]+u[1:,:,:])\n",
        "vc = 0.5e0*(v[:,:-1,:]+v[:,1:,:])\n",
        "wc = 0.5e0*(w[:,:,:-1]+w[:,:,1:])\n",
        "xc = 0.5e0*(x[:-1]+x[1:])\n",
        "yc = 0.5e0*(y[:-1]+y[1:])\n",
        "zc = 0.5e0*(z[:-1]+z[1:])"
      ],
      "metadata": {
        "id": "wEW_jxY1nvqW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nx = 20   # height of highreso_data\n",
        "Ny = 20   # width of highreso_data\n",
        "\n",
        "nx = 10   # height of lowreso_data\n",
        "ny = 10   # width of lowreso_data\n",
        "\n",
        "# xh and xl are numpy array\n",
        "xh = np.empty([int(len(uc[0,0,:])*len(uc[0,:,0])*len(uc[:,0,0])/Nx/Ny),2,Nx,Ny])\n",
        "xl = np.empty_like(xh)\n",
        "\n",
        "b = 0\n",
        "for k in range(len(uc[0,0,:])):\n",
        "  for i in range(0,len(uc[:,0,0]),Nx):\n",
        "    for j in range(0,len(uc[0,:,0]),Ny):\n",
        "      # highreso_data\n",
        "      xh[b,0,:,:] = (uc[i:i+Nx,j:j+Ny,k]-np.mean(uc[:,:,:]))/np.std(uc[:,:,:])\n",
        "      xh[b,1,:,:] = (vc[i:i+Nx,j:j+Ny,k]-np.mean(vc[:,:,:]))/np.std(vc[:,:,:])\n",
        "      # lowreso_data\n",
        "      xl[b,0,:,:] = cv2.resize(cv2.resize(xh[b,0,:,:],(nx,ny),interpolation=cv2.INTER_NEAREST),(Nx,Ny),interpolation=cv2.INTER_NEAREST)\n",
        "      xl[b,1,:,:] = cv2.resize(cv2.resize(xh[b,1,:,:],(nx,ny),interpolation=cv2.INTER_NEAREST),(Nx,Ny),interpolation=cv2.INTER_NEAREST)\n",
        "      b += 1"
      ],
      "metadata": {
        "id": "ZXeQSkqUk1ZM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define my dataset\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, highreso_data, lowreso_data):\n",
        "    self.data = torch.tensor(highreso_data,dtype=torch.float32)\n",
        "    self.targets = torch.tensor(lowreso_data,dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.data[index]\n",
        "    y = self.targets[index]\n",
        "    return x, y\n",
        "\n",
        "dataset = Dataset(xh,xl)  # xh and xl are numpy array\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "print(\"train_dataset size: {}\".format(len(train_dataset)))\n",
        "print(\"test_dataset size: {}\".format(len(test_dataset)))\n",
        "\n",
        "# make dataloader\n",
        "train_batch = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "test_batch = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                         batch_size=50,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "# check minibacth_dataset\n",
        "for highreso_data, lowreso_data in train_batch:\n",
        "    print(\"batch highreso_data size: {}\".format(highreso_data.size()))  # batch size of highreso_data\n",
        "    print(\"highreso data size: {}\".format(highreso_data[0].size()))  # highreso_data size\n",
        "    print(\"batch lowreso_data size: {}\".format(lowreso_data.size()))  # batch size of lowreso_data\n",
        "    print(\"lowreso data size: {}\".format(lowreso_data[1].size()))  # lowreso_data size\n",
        "    break"
      ],
      "metadata": {
        "id": "JFPYC48-WPbg",
        "outputId": "9075d955-6938-45f6-e01d-37db702c41eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset size: 800\n",
            "test_dataset size: 200\n",
            "batch highreso_data size: torch.Size([50, 2, 20, 20])\n",
            "highreso data size: torch.Size([2, 20, 20])\n",
            "batch lowreso_data size: torch.Size([50, 2, 20, 20])\n",
            "lowreso data size: torch.Size([2, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display data\n",
        "def cat_imshow(nx, ny, highreso_data, lowreso_data):\n",
        "  plt.figure(figsize=(9, 7))\n",
        "  for i in range(nx*ny):  # display X * Y data\n",
        "    if i <= 3:\n",
        "      velocity = highreso_data[i]\n",
        "    elif i > 3 and i <= 7:\n",
        "      velocity = lowreso_data[i-4]\n",
        "    speed = np.sqrt(velocity[0,:,:]**2+velocity[1,:,:]**2)\n",
        "    speed = speed.numpy()  # from Tensor to ndarray\n",
        "    x = np.linspace(0,1,len(speed[:,0]))\n",
        "    y = np.linspace(0,1,len(speed[0,:]))\n",
        "    x, y = np.meshgrid(x,y)\n",
        "    plt.subplot(nx, ny, i+1)\n",
        "    plt.contourf(x, y, speed, cmap='turbo', levels=20)\n",
        "    plt.axis('equal')\n",
        "    plt.axis('off')  # delete scale\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "# check data\n",
        "for highreso_data, lowreso_data in train_batch:\n",
        "  cat_imshow(2, 4, highreso_data, lowreso_data)\n",
        "  break"
      ],
      "metadata": {
        "id": "4U_4BRmk0Ct7",
        "outputId": "2206fe0e-a1ee-4d6c-9955-f00f7e68ba69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x700 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RotatedConv1(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(RotatedConv1, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.freeze_conv_parameters()\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "  def rotate_filter(self, k):\n",
        "    return torch.rot90(self.conv.weight, k, dims=(2, 3))\n",
        "\n",
        "  def freeze_conv_parameters(self):\n",
        "    for param in self.conv.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = F.conv2d(x, self.rotate_filter(1), stride=self.stride, padding=self.padding)\n",
        "    x2 = F.conv2d(x, self.rotate_filter(2), stride=self.stride, padding=self.padding)\n",
        "    x3 = F.conv2d(x, self.rotate_filter(3), stride=self.stride, padding=self.padding)\n",
        "    x4 = F.conv2d(x, self.rotate_filter(4), stride=self.stride, padding=self.padding)\n",
        "\n",
        "    return x1,x2,x3,x4\n",
        "\n",
        "class RotatedConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(RotatedConv, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.freeze_conv_parameters()\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "  def rotate_filter(self, k):\n",
        "    return torch.rot90(self.conv.weight, k, dims=(2, 3))\n",
        "\n",
        "  def freeze_conv_parameters(self):\n",
        "    for param in self.conv.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1, x2, x3, x4 = x\n",
        "    x1 = F.conv2d(x1, self.rotate_filter(1), stride=self.stride, padding=self.padding)\n",
        "    x2 = F.conv2d(x2, self.rotate_filter(2), stride=self.stride, padding=self.padding)\n",
        "    x3 = F.conv2d(x3, self.rotate_filter(3), stride=self.stride, padding=self.padding)\n",
        "    x4 = F.conv2d(x4, self.rotate_filter(4), stride=self.stride, padding=self.padding)\n",
        "    return x1,x2,x3,x4\n",
        "\n",
        "class GCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GCNN, self).__init__()\n",
        "    self.conv1 = RotatedConv1(in_channels=2, out_channels=64, kernel_size=9, stride=1, padding=4)\n",
        "    self.conv2 = RotatedConv(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=0)\n",
        "    self.conv3 = RotatedConv(in_channels=32, out_channels=2, kernel_size=5, stride=1, padding=2)\n",
        "  def forward(self, x):\n",
        "    # Z2-P4 conv\n",
        "    x1,x2,x3,x4 = self.conv1(x)\n",
        "    # P4-P4 conv\n",
        "    x1,x2,x3,x4 = self.conv2((x1,x2,x3,x4))\n",
        "    x1,x2,x3,x4 = self.conv3((x1,x2,x3,x4))\n",
        "    # pooling\n",
        "    x1 = F.max_pool2d(x1, kernel_size=2, stride=1, padding=0)\n",
        "    x2 = F.max_pool2d(x2, kernel_size=2, stride=1, padding=0)\n",
        "    x3 = F.max_pool2d(x3, kernel_size=2, stride=1, padding=0)\n",
        "    x4 = F.max_pool2d(x4, kernel_size=2, stride=1, padding=0)\n",
        "    x = torch.max(torch.max(x1,x2),torch.max(x3,x4))\n",
        "    # up-scaling\n",
        "    x = F.interpolate(x,size=(20,20), mode='bicubic',align_corners=False)\n",
        "    return x\n",
        "\n",
        "# use GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = GCNN().to(device)\n",
        "print(net)\n",
        "# check device\n",
        "print(\"Device: {}\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJTbb6vGoLvp",
        "outputId": "8349cb70-fce9-4e19-9527-22421d126640"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNN(\n",
            "  (conv1): RotatedConv1(\n",
            "    (conv): Conv2d(2, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
            "  )\n",
            "  (conv2): RotatedConv(\n",
            "    (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv3): RotatedConv(\n",
            "    (conv): Conv2d(32, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            ")\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "metadata": {
        "id": "QvgCYCHFR6pn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make lists to store MSE\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "# do machine learning\n",
        "epoch = 100\n",
        "for i in range(epoch):\n",
        "  # progress var\n",
        "  print('---------------------------------------------')\n",
        "  print(\"Epoch: {}/{}\".format(i+1, epoch))\n",
        "\n",
        "  # initialize loss\n",
        "  train_loss = 0\n",
        "  test_loss = 0\n",
        "\n",
        "  # train GCNN\n",
        "  net.train()\n",
        "  # load mini batch\n",
        "  i = 0\n",
        "  for highreso_data, lowreso_data in train_batch:\n",
        "    highreso_data.requires_grad_()\n",
        "    lowreso_data.requires_grad_()\n",
        "    # transfer Tensor to GPU\n",
        "    highreso_data = highreso_data.to(device)\n",
        "    lowreso_data = lowreso_data.to(device)\n",
        "\n",
        "    # initialize grad\n",
        "    optimizer.zero_grad()\n",
        "    # calc pred\n",
        "    y_pred = net(lowreso_data)\n",
        "    # calc loss\n",
        "    loss = criterion(y_pred, highreso_data)\n",
        "    # calc grad\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # stock train loss\n",
        "    train_loss += loss.item()\n",
        "    i += 1\n",
        "\n",
        "  # calc mean loss\n",
        "  batch_train_loss = train_loss / len(train_batch)\n",
        "\n",
        "  # evaluate GCNN\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for highreso_data, lowreso_data in test_batch:\n",
        "      # transfer Tensor to GPU\n",
        "      highreso_data = highreso_data.to(device)\n",
        "      lowreso_data = lowreso_data.to(device)\n",
        "      # calc pred\n",
        "      y_pred = net(lowreso_data)\n",
        "      # calc loss\n",
        "      loss = criterion(y_pred, highreso_data)\n",
        "      # stock test loss\n",
        "      test_loss += loss.item()\n",
        "\n",
        "  # calc mean loss\n",
        "  batch_test_loss = test_loss / len(test_batch)\n",
        "\n",
        "  print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n",
        "  print(\"Test_Loss: {:.4f}\".format(batch_test_loss))\n",
        "\n",
        "  train_loss_list.append(batch_train_loss)\n",
        "  test_loss_list.append(batch_test_loss)"
      ],
      "metadata": {
        "id": "aWyx1SvsSFJ-",
        "outputId": "b6418ef6-2667-4e1e-c49c-82f3fa11531a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "Epoch: 1/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 2/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 3/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 4/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 5/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 6/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 7/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 8/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 9/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 10/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 11/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 12/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 13/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 14/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 15/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 16/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 17/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 18/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 19/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 20/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 21/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 22/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 23/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 24/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 25/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 26/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 27/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 28/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 29/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 30/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 31/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 32/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 33/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 34/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 35/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 36/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 37/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 38/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 39/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 40/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 41/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 42/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 43/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 44/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 45/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 46/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 47/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 48/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 49/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 50/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 51/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 52/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 53/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 54/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 55/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 56/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 57/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 58/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 59/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 60/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 61/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 62/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 63/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 64/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 65/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 66/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 67/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 68/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 69/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 70/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 71/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 72/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 73/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 74/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 75/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 76/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 77/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 78/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 79/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 80/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 81/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 82/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 83/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 84/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 85/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 86/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 87/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 88/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 89/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 90/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 91/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 92/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 93/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 94/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 95/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 96/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 97/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 98/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 99/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n",
            "---------------------------------------------\n",
            "Epoch: 100/100\n",
            "Train_Loss: 1.3060\n",
            "Test_Loss: 1.1435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WE1NQfVvEndj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}